# Goal
- To understand the basic Pytorch training pipeline and apply it to simple NLP tasks

# Learning Log

## Day01
- Learned basic tensor operations
- Checked how autograd computes gradients

## Day02
- Implemented a simple nn.Module
- Understood that a model is a function with parameters

## Day03
- Learned how loss functions work
- Observed parameter updates using optimizer
  
## Day04
- Built a full training loop
- Verified that loss decreases during training
  
## Day05
- Text is converted into token indices
  
## Day06
- Used embedding layer to represent words as vectors
  
## Day07
- Applied mean pooling to sentence embeddings
  
## Day08
- Changed learning rate and observed performance differences

## Day09
- To implement a basic NLP classification model

## Day10
- Computes weighted sum of values based on similarity

## Day11
- Implemented scaled dot-product attention

## Day12
- Scaling stabilizes gradients

## Day13
- This repository implements a Transformer encoder from scratch

## Day14
- Implemented multi-head attention

## Day15
- Added positional encoding to preserve order information

## Day16
- Attention + Feed Forward + Residual + LayerNorm

